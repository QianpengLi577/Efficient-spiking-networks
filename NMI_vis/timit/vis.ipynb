{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python38364bitsnnconda9a8bd72d9c7f40db9654d23148b18415",
   "display_name": "Python 3.8.3 64-bit ('SNN': conda)",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "gradient type:  MG\n",
      "device:  cuda:0\n",
      "(13134, 100, 100) (39,) 1.6\n",
      "input dataset shap:  (13134, 100, 39)\n",
      "output dataset shap:  (13134, 100, 61)\n"
     ]
    }
   ],
   "source": [
    "# bi-directional srnn within pkg\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim.lr_scheduler import StepLR,MultiStepLR\n",
    "import math\n",
    "import torch.nn.functional as F\n",
    "from torch.utils import data\n",
    "\n",
    "from SRNN_layers.spike_dense import *#spike_dense,readout_integrator\n",
    "from SRNN_layers.spike_neuron import *#output_Neuron\n",
    "from SRNN_layers.spike_rnn import *# spike_rnn\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('device: ',device)\n",
    "\n",
    "def normalize(data_set,Vmax,Vmin):\n",
    "    return (data_set-Vmin)/(Vmax-Vmin)#+1e-6)\n",
    "\n",
    "train_data = np.load('./f40/train_f40_t100.npy')\n",
    "test_data = np.load('./f40/test_f40_t100.npy')\n",
    "valid_data = np.load('./f40/valid_f40_t100.npy')\n",
    "\n",
    "\n",
    "num_channels = 39\n",
    "use_channels = 39\n",
    "Vmax = np.max(train_data[:,:,:use_channels],axis=(0,1))\n",
    "Vmin = np.min(train_data[:,:,:use_channels],axis=(0,1))\n",
    "print(train_data.shape,Vmax.shape,b_j0_value)\n",
    "\n",
    "train_x = normalize(train_data[:,:,:use_channels],Vmax,Vmin)\n",
    "train_y = train_data[:,:,num_channels:]\n",
    "\n",
    "test_x = normalize(test_data[:,:,:num_channels],Vmax,Vmin)\n",
    "test_y = test_data[:,:,num_channels:]\n",
    "\n",
    "valid_x = normalize(valid_data[:,:,:num_channels],Vmax,Vmin)\n",
    "valid_y = valid_data[:,:,num_channels:]\n",
    "\n",
    "print('input dataset shap: ',train_x.shape)\n",
    "print('output dataset shap: ',train_y.shape)\n",
    "_,seq_length,input_dim = train_x.shape\n",
    "_,_,output_dim = train_y.shape\n",
    "\n",
    "batch_size =16\n",
    "# spike_neuron.b_j0_value = 1.59\n",
    "\n",
    "torch.manual_seed(0)\n",
    "def get_DataLoader(train_x,train_y,batch_size=200):\n",
    "    train_dataset = data.TensorDataset(torch.Tensor(train_x), torch.Tensor(np.argmax(train_y,axis=-1)))\n",
    "    train_loader = data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    return train_loader\n",
    "\n",
    "train_loader = get_DataLoader(train_x,train_y,batch_size=batch_size)\n",
    "test_loader = get_DataLoader(test_x,test_y,batch_size=batch_size)\n",
    "valid_loader = get_DataLoader(valid_x,valid_y,batch_size=batch_size)\n",
    "\n",
    "class RNN_s(nn.Module):\n",
    "    def __init__(self,criterion,device,delay=0):\n",
    "        super(RNN_s, self).__init__()\n",
    "        self.criterion = criterion\n",
    "        self.delay = delay\n",
    "        \n",
    "        #self.network = [input_dim,128,128,256,output_dim]\n",
    "        self.network = [39,256,256,output_dim]\n",
    "\n",
    "\n",
    "        self.rnn_fw1 = spike_rnn(self.network[0],self.network[1],\n",
    "                               tau_initializer='multi_normal',\n",
    "                               tauM=[20,20,20,20],tauM_inital_std=[1,5,5,5],\n",
    "                               tauAdp_inital=[200,200,250,200],tauAdp_inital_std=[5,50,100,50],\n",
    "                               device=device)\n",
    "        \n",
    "        self.rnn_bw1 = spike_rnn(self.network[0],self.network[2],\n",
    "                                tau_initializer='multi_normal',\n",
    "                                tauM=[20,20,20,20],tauM_inital_std=[5,5,5,5],\n",
    "                                tauAdp_inital=[200,200,150,200],tauAdp_inital_std=[5,50,30,10],\n",
    "                                device=device)\n",
    "        \n",
    "\n",
    "        self.dense_mean = readout_integrator(self.network[2]+self.network[1],self.network[3],\n",
    "                                    tauM=3,tauM_inital_std=.1,device=device)\n",
    "        \n",
    "\n",
    "    def forward(self, input,labels=None):\n",
    "        b,s,c = input.shape\n",
    "        self.rnn_fw1.set_neuron_state(b)\n",
    "        self.rnn_bw1.set_neuron_state(b)\n",
    "        self.dense_mean.set_neuron_state(b)\n",
    "        \n",
    "        loss = 0\n",
    "        predictions = []\n",
    "        fw_spikes = []\n",
    "        bw_spikes = []\n",
    "        mean_tensor = 0\n",
    "\n",
    "        for l in range(s*5):\n",
    "            input_fw=input[:,l//5,:].float()\n",
    "            input_bw=input[:,-l//5,:].float()\n",
    "\n",
    "            mem_layer1, spike_layer1 = self.rnn_fw1.forward(input_fw)\n",
    "            mem_layer2, spike_layer2 = self.rnn_bw1.forward(input_bw)\n",
    "            fw_spikes.append(spike_layer1)\n",
    "            bw_spikes.insert(0,spike_layer2)\n",
    "        \n",
    "        for k in range(s*5):\n",
    "            bw_idx = int(k//5)*5 + (4 - int(k%5))\n",
    "            second_tensor = bw_spikes[k]#[bw_idx]\n",
    "            merge_spikes = torch.cat((fw_spikes[k], second_tensor), -1)\n",
    "            mean_tensor += merge_spikes\n",
    "            # mem_layer3  = self.dense_mean(mean_tensor/5.)\n",
    "            if k %5 ==4:\n",
    "                mem_layer3  = self.dense_mean(mean_tensor/5.)# mean or accumulate\n",
    "            \n",
    "                output = F.log_softmax(mem_layer3,dim=-1)#\n",
    "                predictions.append(output.data.cpu().numpy())\n",
    "                if labels is not None:\n",
    "                    loss += self.criterion(output, labels[:, k//5])\n",
    "                mean_tensor = 0\n",
    "    \n",
    "        predictions = torch.tensor(predictions)\n",
    "        fw_npy  = np.mean(np.array([t.detach().cpu().numpy() for t in fw_spikes]))\n",
    "        bw_npy  = np.mean(np.array([t.detach().cpu().numpy() for t in bw_spikes]))\n",
    "        return predictions, [fw_spikes,bw_spikes],(bw_npy+fw_npy)/2.\n",
    "\n",
    "\n",
    "def test(data_loader,after_num_frames=0,is_fr=1):\n",
    "    test_acc = 0.\n",
    "    sum_samples = 0\n",
    "    fr = []\n",
    "    for i, (images, labels) in enumerate(data_loader):\n",
    "        images = images.view(-1, seq_length, input_dim).to(device)\n",
    "        labels = labels.view((-1,seq_length)).long().to(device)\n",
    "        predictions, _,fr_ = model(images)\n",
    "        _, predicted = torch.max(predictions.data, 2)\n",
    "        labels = labels.cpu()\n",
    "        predicted = predicted.cpu().t()\n",
    "        fr.append(fr_)\n",
    "        \n",
    "        test_acc += (predicted == labels).sum()\n",
    "        \n",
    "        sum_samples = sum_samples + predicted.numel()\n",
    "    # print(predicted[1],'\\n',labels[1])\n",
    "    if is_fr:\n",
    "        print('Mean fr: ', np.mean(fr))\n",
    "    return test_acc.data.cpu().numpy() / sum_samples\n",
    "\n",
    "def train(model,loader,optimizer,scheduler=None,num_epochs=10):\n",
    "    best_acc = 0\n",
    "    path = 'model/'  # .pth'\n",
    "    acc_list=[]\n",
    "    print(model.rnn_fw1.b_j0)\n",
    "    for epoch in range(num_epochs):\n",
    "        train_acc = 0\n",
    "        train_loss_sum = 0\n",
    "        sum_samples = 0\n",
    "        for i, (images, labels) in enumerate(loader):\n",
    "            images = images.view(-1, seq_length, input_dim).requires_grad_().to(device)\n",
    "            labels = labels.view((-1,seq_length)).long().to(device)\n",
    "            optimizer.zero_grad()\n",
    "    \n",
    "            predictions, train_loss,fr_ = model(images, labels)\n",
    "            _, predicted = torch.max(predictions.data, 2)\n",
    "            \n",
    "            train_loss.backward()\n",
    "            train_loss_sum += train_loss\n",
    "            optimizer.step()\n",
    "\n",
    "            labels = labels.cpu()\n",
    "            predicted = predicted.cpu().t()\n",
    "            \n",
    "            train_acc += (predicted == labels).sum()\n",
    "            sum_samples = sum_samples + predicted.numel()\n",
    "            torch.cuda.empty_cache()\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "            \n",
    "        train_acc = train_acc.data.cpu().numpy() / sum_samples\n",
    "        valid_acc = test(valid_loader)\n",
    "        \n",
    "        if valid_acc>best_acc and train_acc>0.30:\n",
    "            best_acc = valid_acc\n",
    "            torch.save(model, path+str(best_acc)[:7]+'-bi-srnn-v3_MN-v1.pth')\n",
    "\n",
    "        acc_list.append(train_acc)\n",
    "        print('epoch: {:3d}, Train Loss: {:.4f}, Train Acc: {:.4f},Valid Acc: {:.4f}'.format(epoch,\n",
    "                                                                           train_loss_sum.item()/len(loader)/(seq_length),\n",
    "                                                                           train_acc,valid_acc), flush=True)\n",
    "    return acc_list\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "device: cuda:0\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "RNN_s(\n",
       "  (criterion): NLLLoss()\n",
       "  (rnn_fw1): spike_rnn(\n",
       "    (dense): Linear(in_features=39, out_features=512, bias=True)\n",
       "    (recurrent): Linear(in_features=512, out_features=512, bias=True)\n",
       "  )\n",
       "  (rnn_bw1): spike_rnn(\n",
       "    (dense): Linear(in_features=39, out_features=512, bias=True)\n",
       "    (recurrent): Linear(in_features=512, out_features=512, bias=True)\n",
       "  )\n",
       "  (dense_mean): readout_integrator(\n",
       "    (dense): Linear(in_features=1024, out_features=61, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "num_epochs = 200\n",
    "criterion = nn.NLLLoss()#nn.CrossEntropyLoss()\n",
    "model = RNN_s(criterion=criterion,device=device)\n",
    "# model = torch.load('./model/0.66108-bi-srnn-v3_MN-v1.pth')\n",
    "model = torch.load('./0.66292-bi-srnn-v3_MN-v1.pth')\n",
    "# model = torch.load('./0.65901-bi-srnn-v3_MN-v1.pth')\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"device:\",device)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Mean fr:  0.04215048685935991\n0.6618980963045913\n"
     ]
    }
   ],
   "source": [
    "# with sechdual\n",
    "test_acc = test(valid_loader)\n",
    "print(test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (images, labels) in enumerate(test_loader):\n",
    "    if i ==0:\n",
    "        images = images.view(-1, seq_length, input_dim).to(device)\n",
    "        labels = labels.view((-1,seq_length)).long().to(device)\n",
    "        predictions, states,fr_ = model(images)\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "fw_spike = np.array(states[0])\n",
    "bw_spike = np.array(states[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(500, 16, 1024)"
      ]
     },
     "metadata": {},
     "execution_count": 24
    }
   ],
   "source": [
    "\n",
    "b,_ = fw_spike[0].detach().cpu().numpy().shape\n",
    "spike_np = np.zeros((500,b,512+512))\n",
    "for i in range(500):\n",
    "    spike_np[i,:,:512] = fw_spike[i].detach().cpu().numpy()\n",
    "    spike_np[i,:,512:] = bw_spike[i].detach().cpu().numpy()\n",
    "spike_np.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "spike_count = {'total':[],'fr':[],'per step':[]}\n",
    "spike_count['total'].append([np.mean(np.sum(spike_np,axis=(0,2))),np.max(np.sum(spike_np,axis=(0,2))),np.min(np.sum(spike_np,axis=(0,2)))])\n",
    "spike_count['per step'].append([np.mean(np.sum(spike_np,axis=(2))),np.max(np.sum(spike_np,axis=(2))),np.min(np.sum(spike_np,axis=(2)))])\n",
    "spike_count['fr'].append(np.mean(spike_np))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'total': [[21589.4375, 23229.0, 20550.0]],\n",
       " 'fr': [0.0421668701171875],\n",
       " 'per step': [[43.178875, 102.0, 16.0]]}"
      ]
     },
     "metadata": {},
     "execution_count": 26
    }
   ],
   "source": [
    "spike_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "spike_count = {'total':[],'fr':[],'per step':[]}\n",
    "for i, (images, labels) in enumerate(test_loader):\n",
    "    images = images.view(-1, seq_length, input_dim).to(device)\n",
    "    labels = labels.view((-1,seq_length)).long().to(device)\n",
    "    predictions, states,fr_ = model(images)\n",
    "\n",
    "    fw_spike = np.array(states[0])\n",
    "    bw_spike = np.array(states[1])\n",
    "    b,_ = fw_spike[0].detach().cpu().numpy().shape\n",
    "    spike_np = np.zeros((500,b,512+512))\n",
    "    for i in range(500):\n",
    "        spike_np[i,:,:512] = fw_spike[i].detach().cpu().numpy()\n",
    "        spike_np[i,:,512:] = bw_spike[i].detach().cpu().numpy()\n",
    "    \n",
    "    spike_count['total'].append([np.mean(np.sum(spike_np,axis=(0,2))),np.max(np.sum(spike_np,axis=(0,2))),np.min(np.sum(spike_np,axis=(0,2)))])\n",
    "    spike_count['per step'].append([np.mean(np.sum(spike_np,axis=(2))),np.max(np.sum(spike_np,axis=(2))),np.min(np.sum(spike_np,axis=(2)))])\n",
    "    spike_count['fr'].append(np.mean(spike_np))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(43, 3)"
      ]
     },
     "metadata": {},
     "execution_count": 32
    }
   ],
   "source": [
    "spike_total = np.array(spike_count['total'])\n",
    "spike_total.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(21241.8125, 22959.0, 20185.0)"
      ]
     },
     "metadata": {},
     "execution_count": 33
    }
   ],
   "source": [
    "np.mean(spike_total[0]),np.max(spike_total[1]),np.min(spike_total[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(49.892291666666665, 105.0, 15.0)"
      ]
     },
     "metadata": {},
     "execution_count": 34
    }
   ],
   "source": [
    "spike_per = np.array(spike_count['per step'])\n",
    "np.mean(spike_per[0]),np.max(spike_per[1]),np.min(spike_per[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.042145522953003875"
      ]
     },
     "metadata": {},
     "execution_count": 35
    }
   ],
   "source": [
    "spike_fr = np.array(spike_count['fr'])\n",
    "np.mean(spike_fr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}